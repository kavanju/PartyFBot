# bot.py
import os
import re
import json
import time
import logging
import asyncio
from datetime import datetime, timedelta

import requests
from bs4 import BeautifulSoup

from telegram import (
    Update,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    InputMediaPhoto,
)
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    ContextTypes,
    CallbackQueryHandler,
    filters,
)

# try import g4f (free providers). If not installed, code will still work.
try:
    import g4f
    G4F_AVAILABLE = True
except Exception:
    G4F_AVAILABLE = False

from keep_alive import keep_alive

# ---------- –ö–æ–Ω—Ñ–∏–≥ (—Å—Ç–∞–≤—å –≤ env –Ω–∞ Render) ----------
BOT_TOKEN = os.getenv("BOT_TOKEN")  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
KASPI_LINK = os.getenv("KASPI_LINK", "https://pay.kaspi.kz/pay/sav8emzy")
PAYPAL_LINK = os.getenv("PAYPAL_LINK", "https://paypal.me/yourpaypal/1")
FREE_LIMIT = int(os.getenv("FREE_LIMIT", "1"))

USERS_FILE = "users.json"
CACHE_FILE = "cache.json"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"

# ---------- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ----------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------- –•—Ä–∞–Ω–∏–ª–∏—â–∞ ----------
def load_json(path, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return default
    except Exception:
        logger.exception("load_json")
        return default

def save_json(path, data):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        logger.exception("save_json")

users = load_json(USERS_FILE, {})   # key = user_id str -> {lang, free_used:int, paid_until: iso str}
cache = load_json(CACHE_FILE, {})   # key = lang||normalized_query -> card dict

# ---------- –¢–µ–∫—Å—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã (RU/KZ/EN) ----------
TEXT = {
    "choose_lang": {"ru": "–ü—Ä–∏–≤–µ—Ç! –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫:", "kz": "–°”ô–ª–µ–º! –¢—ñ–ª–¥—ñ —Ç–∞“£–¥–∞“£—ã–∑:", "en": "Hi! Choose language:"},
    "ask_desc": {
        "ru": "–û–ø–∏—à–∏—Ç–µ –º–µ—Å—Ç–æ: –≥–æ—Ä–æ–¥, —Ç–∏–ø (–±–∞—Ä/—Ä–µ—Å—Ç–æ—Ä–∞–Ω/–∫–ª—É–±), –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, –±—é–¥–∂–µ—Ç.",
        "kz": "–û—Ä—ã–Ω–¥—ã —Å–∏–ø–∞—Ç—Ç–∞“£—ã–∑: “õ–∞–ª–∞, —Ç–∏–ø (–±–∞—Ä/—Ä–µ—Å—Ç–æ—Ä–∞–Ω/–∫–ª—É–±), –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞, –±—é–¥–∂–µ—Ç.",
        "en": "Describe the place: city, type (bar/restaurant/club), vibe, budget."
    },
    "free_used": {
        "ru": "–£ –≤–∞—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å. –ß—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å ‚Äî –æ–ø–ª–∞—Ç–∏—Ç–µ –∏ –ø—Ä–∏—à–ª–∏—Ç–µ —á–µ–∫ (—Å–∫—Ä–∏–Ω):",
        "kz": "–°—ñ–∑–¥—ñ“£ —Ç–µ–≥—ñ–Ω —Å“±—Ä–∞—É—ã“£—ã–∑ –ø–∞–π–¥–∞–ª–∞–Ω—ã–ª–¥—ã. –ñ–∞–ª“ì–∞—Å—Ç—ã—Ä—É “Ø—à—ñ–Ω —Ç”©–ª–µ“£—ñ–∑ –∂”ô–Ω–µ —á–µ–∫—Ç—ñ“£ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã–Ω –∂—ñ–±–µ—Ä—ñ“£—ñ–∑:",
        "en": "You used your free request. To continue ‚Äî pay and send the receipt (screenshot):"
    },
    "pay_options": {
        "ru": f"üá∞üáø Kaspi Pay ‚Äî 300‚Ç∏ / 48—á: {KASPI_LINK}\nüåç PayPal ‚Äî $1 / 48—á: {PAYPAL_LINK}",
        "kz": f"üá∞üáø Kaspi Pay ‚Äî 300‚Ç∏ / 48—Å–∞“ì: {KASPI_LINK}\nüåç PayPal ‚Äî $1 / 48—Å–∞“ì: {PAYPAL_LINK}",
        "en": f"üá∞üáø Kaspi Pay ‚Äî 300‚Ç∏ / 48h: {KASPI_LINK}\nüåç PayPal ‚Äî $1 / 48h: {PAYPAL_LINK}"
    },
    "send_receipt": {
        "ru": "–ü–æ—Å–ª–µ –æ–ø–ª–∞—Ç—ã –ø—Ä–∏—à–ª–∏—Ç–µ —Å–∫—Ä–∏–Ω —á–µ–∫–∞ (—Ñ–æ—Ç–æ/–¥–æ–∫—É–º–µ–Ω—Ç). –Ø –≤—ã–ø–æ–ª–Ω—é –∏–º–∏—Ç–∞—Ü–∏—é –ø—Ä–æ–≤–µ—Ä–∫–∏ (‚âà30—Å).",
        "kz": "–¢”©–ª–µ–º–Ω–µ–Ω –∫–µ–π—ñ–Ω —á–µ–∫—Ç—ñ“£ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã–Ω –∂—ñ–±–µ—Ä—ñ“£—ñ–∑. –ú–µ–Ω —Ç–µ–∫—Å–µ—Ä—É–¥—ñ —ç–º—É–ª—è—Ü–∏—è–ª–∞–π–º—ã–Ω (‚âà30—Å).",
        "en": "After payment send receipt screenshot (photo/doc). I'll fake-check it (‚âà30s)."
    },
    "checking_receipt": {"ru": "–ü—Ä–æ–≤–µ—Ä—è—é —á–µ–∫ ‚Äî –ø–æ–¥–æ–∂–¥–∏—Ç–µ 30 —Å–µ–∫—É–Ω–¥...", "kz": "–ß–µ–∫—Ç—ñ —Ç–µ–∫—Å–µ—Ä—ñ–ø –∂–∞—Ç—ã—Ä–º—ã–Ω ‚Äî 30 —Å–µ–∫—É–Ω–¥ –∫“Ø—Ç—ñ“£—ñ–∑...", "en": "Checking receipt ‚Äî please wait 30 seconds..."},
    "access_granted": {"ru": "‚úÖ –û–ø–ª–∞—Ç–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ (—ç–º—É–ª—è—Ü–∏—è). –î–æ—Å—Ç—É–ø –Ω–∞ 48 —á–∞—Å–æ–≤.", "kz": "‚úÖ –¢”©–ª–µ–º —Ä–∞—Å—Ç–∞–ª–¥—ã (—ç–º—É–ª—è—Ü–∏—è). 48 —Å–∞“ì–∞—Ç“õ–∞ “õ–æ–ª –∂–µ—Ç—ñ–º–¥—ñ.", "en": "‚úÖ Payment confirmed (fake). Access for 48 hours."},
    "error": {"ru": "–û—à–∏–±–∫–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", "kz": "“ö–∞—Ç–µ, –∫–µ–π—ñ–Ω—ñ—Ä–µ–∫ –∫”©—Ä—ñ“£—ñ–∑.", "en": "Error, try later."},
}

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def user_key(uid): return str(uid)
def get_lang(uid): return users.get(user_key(uid), {}).get("lang", "ru")

def has_paid(uid):
    u = users.get(user_key(uid))
    if not u: return False
    pu = u.get("paid_until")
    if not pu: return False
    try:
        return datetime.fromisoformat(pu) > datetime.utcnow()
    except Exception:
        return False

def set_paid_48h(uid):
    users.setdefault(user_key(uid), {})
    users[user_key(uid)]["paid_until"] = (datetime.utcnow() + timedelta(hours=48)).isoformat()
    save_json(USERS_FILE, users)

def inc_free_used(uid):
    users.setdefault(user_key(uid), {})
    users[user_key(uid)]["free_used"] = users[user_key(uid)].get("free_used", 0) + 1
    save_json(USERS_FILE, users)

# ---------- DuckDuckGo image (best-effort) ----------
def ddg_image_first(query):
    try:
        headers = {"User-Agent": USER_AGENT}
        r = requests.get("https://duckduckgo.com/", params={"q": query}, headers=headers, timeout=10)
        vqd = re.search(r"vqd='([^']+)'", r.text)
        vqd = vqd.group(1) if vqd else None
        params = {"q": query, "o": "json", "vqd": vqd}
        r2 = requests.get("https://duckduckgo.com/i.js", params=params, headers=headers, timeout=10)
        data = r2.json()
        res = data.get("results", [])
        if res:
            return res[0].get("image")
    except Exception:
        logger.exception("ddg_image_first")
    return None

# ---------- Geocode via OSM Nominatim ----------
def geocode_osm(query):
    try:
        r = requests.get("https://nominatim.openstreetmap.org/search", params={"q": query, "format": "json", "limit": 1}, headers={"User-Agent": USER_AGENT}, timeout=10)
        data = r.json()
        if data:
            return {"display_name": data[0].get("display_name"), "lat": float(data[0]["lat"]), "lon": float(data[0]["lon"])}
    except Exception:
        logger.exception("geocode_osm")
    return None

def osm_static_map_url(lat, lon, w=600, h=300, zoom=15):
    return f"https://staticmap.openstreetmap.de/staticmap.php?center={lat},{lon}&zoom={zoom}&size={w}x{h}&markers={lat},{lon},red-pushpin"

def osm_map_link(lat, lon): return f"https://www.openstreetmap.org/?mlat={lat}&mlon={lon}#map=17/{lat}/{lon}"

# ---------- Best-effort –æ—Ç–∑—ã–≤—ã: –ø–∞—Ä—Å–∏–Ω–≥ Google Search / TripAdvisor / Yelp ----------
def fetch_reviews_best_effort(name, city=None, max_reviews=3):
    headers = {"User-Agent": USER_AGENT}
    reviews = []
    query = f"{name} {city}" if city else name
    # 1) Google '–æ—Ç–∑—ã–≤—ã' –≤—ã–¥–∞—á–∞ (snippet)
    try:
        r = requests.get("https://www.google.com/search", params={"q": f"{query} –æ—Ç–∑—ã–≤—ã", "hl": "ru"}, headers=headers, timeout=8)
        soup = BeautifulSoup(r.text, "html.parser")
        # —Å–æ–±—Ä–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        for div in soup.select("div"):
            txt = div.get_text(separator=" ", strip=True)
            if txt and len(txt) > 40 and "–æ—Ç–∑—ã–≤" not in txt.lower():
                reviews.append(txt.strip())
            if len(reviews) >= max_reviews:
                return reviews[:max_reviews]
    except Exception:
        logger.exception("google search parse")

    # 2) TripAdvisor search page snippets
    try:
        r = requests.get("https://www.tripadvisor.com/Search", params={"q": query}, headers=headers, timeout=8)
        soup = BeautifulSoup(r.text, "html.parser")
        for p in soup.find_all("p"):
            txt = p.get_text(strip=True)
            if txt and len(txt) > 60:
                reviews.append(txt)
            if len(reviews) >= max_reviews:
                return reviews[:max_reviews]
    except Exception:
        logger.exception("tripadvisor parse")

    # 3) Yelp search page snippets (may be limited by region)
    try:
        r = requests.get("https://www.yelp.com/search", params={"find_desc": name, "find_loc": city or ""}, headers=headers, timeout=8)
        soup = BeautifulSoup(r.text, "html.parser")
        for p in soup.find_all("p"):
            txt = p.get_text(strip=True)
            if txt and len(txt) > 40:
                reviews.append(txt)
            if len(reviews) >= max_reviews:
                return reviews[:max_reviews]
    except Exception:
        logger.exception("yelp parse")

    return reviews[:max_reviews]

# ---------- g4f helpers: analyze reviews -> atmosphere/age/summary ----------
def g4f_analyze_text(reviews_text):
    if not G4F_AVAILABLE or not reviews_text:
        return None
    try:
        prompt = (
            f"–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –æ—Ç–∑—ã–≤–æ–≤:\n{reviews_text}\n\n"
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π: 1) –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ (–∫–æ—Ä–æ—Ç–∫–æ), 2) –°—Ä–µ–¥–Ω–∏–π –≤–æ–∑—Ä–∞—Å—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π (–ø—Ä–∏–º–µ—Ä–Ω–æ), 3) –û–¥–Ω–∞-–¥–≤–µ —Å—Ç—Ä–æ–∫–∏ summary."
            "–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ: ATMOSPHERE: ...; AGE: ...; SUMMARY: ..."
        )
        resp = None
        try:
            resp = g4f.ChatCompletion.create(model="gpt-4o-mini", messages=[{"role":"user","content":prompt}])
        except Exception:
            try:
                resp = g4f.chat_completion.create(model="gpt-4o-mini", messages=[{"role":"user","content":prompt}])
            except Exception:
                resp = None
        text = resp if isinstance(resp, str) else (str(resp) if resp else "")
        atm = re.search(r"ATMOSPHERE[:\-]\s*(.+)", text, re.I)
        age = re.search(r"AGE[:\-]\s*(.+)", text, re.I)
        summ = re.search(r"SUMMARY[:\-]\s*(.+)", text, re.I | re.S)
        return {
            "atmosphere": atm.group(1).strip() if atm else None,
            "age": age.group(1).strip() if age else None,
            "summary": summ.group(1).strip() if summ else text.strip()
        }
    except Exception:
        logger.exception("g4f_analyze_text")
        return None

# ---------- –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –Ω–µ—Ç g4f) ----------
def heuristic_analysis(query):
    q
